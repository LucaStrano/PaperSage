configs:

  - chainlit_config:
      - host: localhost
      - port: 8000

  - qdrant_config:
      - text_collection_name: paper_texts
      - image_collection_name: paper_images
      
  - embedding_config: # If you want to change model, also change the other settings accordingly
      - model: nomic-embed-text:137m-v1.5-fp16 # ollama model id
      - max_seq_length: 8192
      - output_length: 768
      - use_prefix: True
      - query_prefix: "search_query: "
      - document_prefix: "search_document: "
      - tokenizer: nomic-ai/nomic-embed-text-v1.5 # huggingface model id
      - embed_images: True # Embed images if true (do not change this, causes unexpected behavior)
      - image_model: nomic-ai/nomic-embed-vision-v1.5 # huggingface model id
      - use_same_output_length: True # use the same output length for text and image embeddings
      - img_output_length: null # set if use_same_output_length is False

  - chunking_config:
      - use_emb_model_max_seq_length: False
      - chunk_size: 500 # set if use_emb_model_max_seq_length is False
      - chunk_size_penalty: 20 # decreases max chunk size by amount, used to append section titles to chunks
      - chunk_overlap_percent: 0.2
      - add_section_titles: True # add section titles to chunks

  - agent_config:
      - model_name: openai/gpt-4o-mini # litellm model id
      - temperature: 0.4
      - search_type: similarity # or mmr
      - k: 5
      - fetch_k: 20
      - msg_history_len: 5
      - img_sim_threshold: 0.4